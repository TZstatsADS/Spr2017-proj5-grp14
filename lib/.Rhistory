knitr::opts_chunk$set(echo = TRUE)
cv.function <- function(X.train, y.train, d){
library(gbm)
train <- function(dat_train, label_train, par=NULL){
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=dat_train, y=label_train,
n.trees=1000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
return(list(fit=fit_gbm, iter=best_iter))
}
test <- function(fit_train, dat_test){
pred <- predict(fit_train$fit, newdata=dat_test,
n.trees=fit_train$iter, type="response")
return(as.numeric(pred> 0.5))
}
n <- length(y.train)
train.data <- X.train[1:40000,]
train.label <- y.train[1:40000]
test.data <- X.train[40001:50000,]
test.label <- y.train[40001:50000]
par <- list(depth=d)
fit <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error[i] <- mean(pred != test.label)
return(list(err=mean(cv.error),pred=pred))
}
e<-c()
gbmbase<-list()
gbmlabel<-list()
result<-data.frame()
i<-1
cl <- makeCluster(getOption("cl.cores", 8))
tfeature1<-read.csv("../output/feature.csv",header = TRUE)[,-1]
tfeature1<-read.csv("../output/feature.csv",header = TRUE)[,-1]
tfeature2<-read.csv("../output/feature_yiwenci.csv",header = TRUE)[,-1]
tfeature3<-cbind(tfeature1,tfeature2)
tfeature3<-cbind(tfeature1,tfeature2)
tfeature4<-read.csv("../output/feature_lyq.csv",header = TRUE)[,-1]
tfeature5<-(read.csv("../output/parsing.Q1.csv",header = TRUE)-read.csv("../output/parsing.Q2.csv",header = TRUE))[,-1]
tfeature5<-(read.csv("../output/parsing.Q1.csv",header = TRUE)-read.csv("../output/parsing.Q2.csv",header = TRUE))[,-1]
tfeature6<-cbind(tfeature3,tfeature4)
tfeature7<-read.csv("../output/allfeatures.csv",header = TRUE)[,-1]
tfeature3<-cbind(tfeature1,tfeature2)
tfeature4<-read.csv("../output/feature_lyq.csv",header = TRUE)[,-1]
tfeature5<-(read.csv("../output/parsing.Q1.csv",header = TRUE)-read.csv("../output/parsing.Q2.csv",header = TRUE))[,-1]
tfeature6<-cbind(tfeature3,tfeature4)
tfeature7<-read.csv("../output/allfeatures.csv",header = TRUE)[,-1]
tfeature<-list(tfeature1,tfeature2,tfeature3,tfeature4,tfeature5,tfeature6,tfeature7)
if(!require("gbm")){
install.packages("gbm")
}
library(gbm)
train <- read.csv("../data/train.csv", header = T, as.is = T)
label<-train[1:50000,6]
library(tm)
library(tm)
library(qdap)
library(plyr)
library(tm.lexicon.GeneralInquirer)
library(tm.plugin.sentiment)
cv.function <- function(X.train, y.train, d){
library(gbm)
train <- function(dat_train, label_train, par=NULL){
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=dat_train, y=label_train,
n.trees=1000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
return(list(fit=fit_gbm, iter=best_iter))
}
test <- function(fit_train, dat_test){
pred <- predict(fit_train$fit, newdata=dat_test,
n.trees=fit_train$iter, type="response")
return(as.numeric(pred> 0.5))
}
n <- length(y.train)
train.data <- X.train[1:40000,]
train.label <- y.train[1:40000]
test.data <- X.train[40001:50000,]
test.label <- y.train[40001:50000]
par <- list(depth=d)
fit <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error[i] <- mean(pred != test.label)
return(list(err=mean(cv.error),pred=pred))
}
e<-c()
gbmbase<-list()
gbmlabel<-list()
result<-data.frame()
i
cl <- makeCluster(getOption("cl.cores", 8))
r<-parLapply(cl, 1:16, cv.function,X.train=tfeature[[i]], y.train=label)
r<-parLapply(cl, 1:16, cv.function,X.train=tfeature[[i]], y.train=label)
cv.function <- function(X.train, y.train, d){
library(gbm)
train <- function(dat_train, label_train, par=NULL){
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=dat_train, y=label_train,
n.trees=1000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
return(list(fit=fit_gbm, iter=best_iter))
}
test <- function(fit_train, dat_test){
pred <- predict(fit_train$fit, newdata=dat_test,
n.trees=fit_train$iter, type="response")
return(as.numeric(pred> 0.5))
}
n <- length(y.train)
train.data <- X.train[1:40000,]
train.label <- y.train[1:40000]
test.data <- X.train[40001:50000,]
test.label <- y.train[40001:50000]
par <- list(depth=d)
fit <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error <- mean(pred != test.label)
return(list(err=cv.error,pred=pred))
}
r<-parLapply(cl, 1:16, cv.function,X.train=tfeature[[i]], y.train=label)
cv.function <- function(X.train, y.train, d){
library(gbm)
train <- function(dat_train, label_train, par=NULL){
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=dat_train, y=label_train,
n.trees=1000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
return(list(fit=fit_gbm, iter=best_iter))
}
test <- function(fit_train, dat_test){
pred <- predict(fit_train$fit, newdata=dat_test,
n.trees=fit_train$iter, type="response")
return(as.numeric(pred> 0.5))
}
n <- length(y.train)
train.data <- X.train[1:40000,]
train.label <- y.train[1:40000]
test.data <- X.train[40001:50000,]
test.label <- y.train[40001:50000]
par <- list(depth=d)
fit <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error <- mean(pred != test.label)
return(list(err=cv.error,pred=pred))
}
e<-c()
gbmbase<-list()
gbmlabel<-list()
result<-data.frame()
cl <- makeCluster(getOption("cl.cores", 8))
r
i
cv.function <- function(X.train, y.train, d){
library(gbm)
train <- function(dat_train, label_train, par=NULL){
if(is.null(par)){
depth <- 3
} else {
depth <- par$depth
}
fit_gbm <- gbm.fit(x=dat_train, y=label_train,
n.trees=1000,
distribution="bernoulli",
interaction.depth=depth,
bag.fraction = 0.5,
verbose=FALSE)
best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
return(list(fit=fit_gbm, iter=best_iter))
}
test <- function(fit_train, dat_test){
pred <- predict(fit_train$fit, newdata=dat_test,
n.trees=fit_train$iter, type="response")
return(as.numeric(pred> 0.5))
}
n <- length(y.train)
train.data <- X.train[1:40000,]
train.label <- y.train[1:40000]
test.data <- X.train[40001:50000,]
test.label <- y.train[40001:50000]
par <- list(depth=d)
fit <- train(train.data, train.label, par)
pred <- test(fit, test.data)
cv.error<- mean(pred != test.label)
return(list(err=cv.error,pred=pred))
}
r<-parLapply(cl, 1:4, cv.function,X.train=tfeature[[i]], y.train=label)
err<-c()
for (j in 1:16){
err<-c(err,r[[j]][[1]])
}
for (j in 1:4){
err<-c(err,r[[j]][[1]])
}
e[i]<-min(err)
d<-which.min(err)
gbmlabel[[i]]<-r[[d]][[2]]
gbmlabel[[1]]
x<-c(1,3,6,7)
e<-c()
gbmbase<-list()
gbmlabel<-list()
result<-data.frame()
x<-c(1,3,6,7)
for (i in x){
cl <- makeCluster(getOption("cl.cores", 8))
r<-parLapply(cl, 1:16, cv.function,X.train=tfeature[[i]], y.train=label)
err<-c()
for (j in 1:16){
err<-c(err,r[[j]][[1]])
}
e[i]<-min(err)
d<-which.min(err)
gbmlabel[[i]]<-r[[d]][[2]]
# gbmbase[[i]] <- gbm.fit(x=tfeature[[i]], y=label,
#                    n.trees=1000,
#                    distribution="bernoulli",
#                    interaction.depth=d,
#                    bag.fraction = 0.5,
#                    verbose=FALSE)
}
e
save(e,file="../output/gbmerror.RData")
save(gbmlabel,file="../output/gbmlabel.RData")
save(list(error=e[7],label=gbmlabel[[7]],file="../output/gbmresults.RData"))
gbmlabel[[7]]
save(list(error=e[7],label=gbmlabel[[7]]),file="../output/gbmresults.RData")
results<-list(error=e[7],label=gbmlabel[[7]])
results[[1]]
results[[2]]
save(results,file="../output/gbmresults.RData")
