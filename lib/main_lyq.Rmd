---
title: "main_lyq"
author: "Yaqin Li (yl3578)"
date: "4/24/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##Input and packages

```{r}
train <- read.csv("../data/train.csv", header = T, as.is = T)
Q1<-train[1:50000,4]
Q2<-train[1:50000,5]
label<-train[1:50000,6]

library(tm)
library(qdap)
library(plyr)
#install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
#library(devtools)
#install_github("mannau/tm.plugin.sentiment")
library(tm.plugin.sentiment)

Q1.doc<-Corpus(VectorSource(Q1))
Q2.doc<-Corpus(VectorSource(Q2))
```

##Transform and word count

```{r}
Q1.doc<-tm_map(Q1.doc,content_transformer(tolower))
Q2.doc<-tm_map(Q2.doc,content_transformer(tolower))

text.count<-function(sent){
  text<-sent[[1]]
  punc<-length(gregexpr("[.|!|?|;|:|,]",text)[[1]])
  modal<-length(gregexpr("will|can|shall|must|should|would|could|may|might",text)[[1]])
  neg<-length(gregexpr("never|not|non|rare|no|neither|seldom|hardly",text)[[1]])
  word<-strsplit(text," ")
  return(list(punc=punc,modal=modal,neg=neg,word=length(word[[1]])))
}



Q1.num<-llply(Q1.doc,text.count)
Q1.num<-ldply(Q1.num,unlist)

Q2.num<-llply(Q2.doc,text.count)
Q2.num<-ldply(Q2.num,unlist)

```




##Sentiment analysis
```{r}
pos.score <- tm_term_score(TermDocumentMatrix(Q1.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Positiv")) 

neg.score <- tm_term_score(TermDocumentMatrix(Q1.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Negativ")) 

Q1.sentiment <- data.frame(positive = pos.score, negative = neg.score)
Q1.sentiment <- transform(Q1.sentiment, net = positive - negative)

pos.score <- tm_term_score(TermDocumentMatrix(Q2.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Positiv")) 

neg.score <- tm_term_score(TermDocumentMatrix(Q2.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Negativ")) 

Q2.sentiment <- data.frame(positive = pos.score, negative = neg.score)
Q2.sentiment <- transform(Q2.sentiment, net = positive - negative)
```


##feature extraction

```{r}
feature<-data.frame(punc=Q2.num$punc-Q1.num$punc)
feature$modal<-abs(Q2.num$modal-Q1.num$modal)/(Q1.num$word+Q2.num$word)
feature$length<-abs(Q2.num$word-Q1.num$word)/(Q1.num$word+Q2.num$word)
feature$neg<-abs(Q2.num$neg-Q1.num$neg)/(Q1.num$word+Q2.num$word)
feature$sentiment<-abs(Q2.sentiment$net-Q1.sentiment$net)
feature$positive<-abs(Q2.sentiment$positive-Q1.sentiment$positive)
feature$negative<-abs(Q2.sentiment$negative-Q1.sentiment$negative)

write.csv(feature,file = "../output/feature_lyq.csv")
```


##combine features

```{r}
tfeature1<-read.csv("../output/feature.csv",header = TRUE)[,-1]
tfeature2<-read.csv("../output/feature_yiwenci.csv",header = TRUE)[,-1]
tfeature3<-cbind(tfeature1,tfeature2)
tfeature4<-read.csv("../output/feature_lyq.csv",header = TRUE)[,-1]
tfeature5<-(read.csv("../output/parsing.Q1.csv",header = TRUE)-read.csv("../output/parsing.Q2.csv",header = TRUE))[,-1]
tfeature6<-cbind(tfeature3,tfeature4)
tfeature7<-read.csv("../output/allfeatures.csv",header = TRUE)[,-1]

tfeature<-list(tfeature1,tfeature2,tfeature3,tfeature4,tfeature5,tfeature6,tfeature7)
#write.csv(tfeature,file="../output/allfeatures.csv")
```

##train model (gbm)

```{r}
if(!require("gbm")){
  install.packages("gbm")
}

library(gbm)


cv.function <- function(X.train, y.train, d){
  
  library(gbm)
  train <- function(dat_train, label_train, par=NULL){
  if(is.null(par)){
    depth <- 3
  } else {
    depth <- par$depth
  }
  
  fit_gbm <- gbm.fit(x=dat_train, y=label_train,
                     n.trees=1000,
                     distribution="bernoulli",
                     interaction.depth=depth, 
                     bag.fraction = 0.5,
                     verbose=FALSE)
  best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
  
  return(list(fit=fit_gbm, iter=best_iter))
  }
  
  test <- function(fit_train, dat_test){
  
  pred <- predict(fit_train$fit, newdata=dat_test, 
                  n.trees=fit_train$iter, type="response")
  
  return(as.numeric(pred> 0.5))
  }

  
  n <- length(y.train)
  
  
    train.data <- X.train[1:40000,]
    train.label <- y.train[1:40000]
    test.data <- X.train[40001:50000,]
    test.label <- y.train[40001:50000]
    
    par <- list(depth=d)
    fit <- train(train.data, train.label, par)
    
    pred <- test(fit, test.data)  
    cv.error<- mean(pred != test.label)  
    
  			
  return(list(err=cv.error,pred=pred))
  
}






e<-c()
gbmbase<-list()
gbmlabel<-list()
result<-data.frame()
x<-c(1,3,6,7)
for (i in x){
  cl <- makeCluster(getOption("cl.cores", 8))
  r<-parLapply(cl, 1:16, cv.function,X.train=tfeature[[i]], y.train=label)
  err<-c()
  for (j in 1:16){
    err<-c(err,r[[j]][[1]])
  }
  e[i]<-min(err)
  d<-which.min(err)
  gbmlabel[[i]]<-r[[d]][[2]]
  # gbmbase[[i]] <- gbm.fit(x=tfeature[[i]], y=label,
  #                    n.trees=1000,
  #                    distribution="bernoulli",
  #                    interaction.depth=d, 
  #                    bag.fraction = 0.5,
  #                    verbose=FALSE)
}
save(gbmbase,file="../output/gbmmodel.RData")
save(e,file="../output/gbmerror.RData")
save(gbmlabel,file="../output/gbmlabel.RData")
```


##sift features (adaboost)

```{r}
agg_class<-function(x,alpha,allpars){
  I<-nrow(allpars)
  c_hat<-0
  for(i in 1:I){
    c_hat<-c_hat+alpha[i]*classify(x,allpars[i,])
  }
  return(sign(c_hat))
}

train<-function(x,w,y){
  l<-length(y)
  c<-matrix(rep(NA,3*ncol(x)),ncol=3,nrow = ncol(x))
  for(j in 2:ncol(x)){
    or<-order(x[,j])
    x.or<-x[or,j]
    y.or<-y[or]
    w.or<-w[or]
    w.cum<-rep(NA,l)
    for(i in 1:l){
      w.cum[i]<-sum(w.or[1:i]*y.or[1:i])
    }
    k1<-which.min(w.cum)
    k2<-which.max(w.cum)
    if(sum(x.or==x.or[k1])>1){
      k1<-max(1,k1-1)
    }
    if(sum(x.or==x.or[k2])>1){
      k2<-min(k2+sum(x.or==x.or[k2]),length(x.or))
    }
    c1<-classify(x,c(j,x.or[k1],1))
    c2<-classify(x,c(j,x.or[k2],-1))
    if(sum(w*(c1!=y))>sum(w*(c2!=y))){
      c[j,]<-c(sum(w*(c2!=y)),x.or[min(k2,l)],-1)
    }
    else{
    c[j,]<-c(sum(w*(c1!=y)),x.or[k1],1)}
  }
  best.j<-which.min(c[,1])
  pars<-c(best.j,c[best.j,2:3])
  return(pars)
}

classify<-function(x,pars){
  j<-pars[1]
  s<-ifelse(x[j]>pars[2],pars[3],-pars[3])
  return(s)
}

data<-tfeature7
group<-ifelse(label==0,-1,1)
data<-cbind(group,data)



AdaBoost<-function(B){
  
n <- nrow(data)
K <- 5
t.e<-rep(NA,K)
v.e<-rep(NA,K)

folds <- sample(rep(1:K, each = n/K))
for (i in 1:K) {
  t.x <- data[folds != i,-1 ]
  v.x <- data[folds == i,-1 ]
  t.y <- data[folds != i,1 ]
  v.y <- data[folds == i,1 ]
  l<-nrow(t.x)
  w<-rep(1/l,l)
  alpha<-rep(NA,B)
  pars<-matrix(rep(NA,B*3),ncol=3,nrow=B)
  
  for(b in 1:B){
    pars[b,]<-train(t.x,w,t.y)
    c<-classify(t.x,pars[b,])
    e<-sum(w*(c!=t.y))/sum(w)
    alpha[b]<-log((1-e)/e)
    w<-w*exp(alpha[b]*(c!=t.y))
  }
  t.c<-agg_class(t.x,alpha,pars)
  v.c<-agg_class(v.x,alpha,pars)
  t.e[i]<-mean(t.c!=t.y)
  v.e[i]<-mean(v.c!=v.y)
}
t.error<-mean(t.e)
v.error<-mean(v.e)
return(list(t.error,v.error))
}



B<-10
predict.error<-rep(NA,B)
train.error<-rep(NA,B)
for(i in 1:B){
  error<-AdaBoost(i)
  predict.error[i]<-error[[2]]
  train.error[i]<-error[[1]]
}
plot(1:B,predict.error,type="l",ylim=c(0,0.20),xlab="number of weak learners",ylab="error rate",col=2)
lines(1:B,train.error,col=3)
legend("topright",legend=c("predict error","train error"),fill=2:3)



```


