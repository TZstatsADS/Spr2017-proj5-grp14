---
title: "main_lyq"
author: "Yaqin Li (yl3578)"
date: "4/24/2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##Input and packages

```{r}
train <- read.csv("../data/train.csv", header = T, as.is = T)
Q1<-train[1:50000,4]
Q2<-train[1:50000,5]
label<-train[1:50000,5]

library(tm)
library(qdap)
library(plyr)
#install.packages("tm.lexicon.GeneralInquirer", repos="http://datacube.wu.ac.at", type="source")
library(tm.lexicon.GeneralInquirer)
#library(devtools)
#install_github("mannau/tm.plugin.sentiment")
library(tm.plugin.sentiment)

Q1.doc<-Corpus(VectorSource(Q1))
Q2.doc<-Corpus(VectorSource(Q2))
```

##Transform and word count

```{r}
Q1.doc<-tm_map(Q1.doc,content_transformer(tolower))
Q2.doc<-tm_map(Q2.doc,content_transformer(tolower))

text.count<-function(sent){
  text<-sent[[1]]
  punc<-length(gregexpr("[.|!|?|;|:|,]",text)[[1]])
  modal<-length(gregexpr("will|can|shall|must|should|would|could|may|might",text)[[1]])
  neg<-length(gregexpr("never|not|non|rare|no|neither|seldom|hardly",text)[[1]])
  word<-strsplit(text," ")
  return(list(punc=punc,modal=modal,neg=neg,word=length(word[[1]])))
}



Q1.num<-llply(Q1.doc,text.count)
Q1.num<-ldply(Q1.num,unlist)

Q2.num<-llply(Q2.doc,text.count)
Q2.num<-ldply(Q2.num,unlist)

```




##Sentiment analysis
```{r}
pos.score <- tm_term_score(TermDocumentMatrix(Q1.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Positiv")) 

neg.score <- tm_term_score(TermDocumentMatrix(Q1.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Negativ")) 

Q1.sentiment <- data.frame(positive = pos.score, negative = neg.score)
Q1.sentiment <- transform(Q1.sentiment, net = positive - negative)

pos.score <- tm_term_score(TermDocumentMatrix(Q2.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Positiv")) 

neg.score <- tm_term_score(TermDocumentMatrix(Q2.doc, control = list(removePunctuation = TRUE)), 
                           terms_in_General_Inquirer_categories("Negativ")) 

Q2.sentiment <- data.frame(positive = pos.score, negative = neg.score)
Q2.sentiment <- transform(Q2.sentiment, net = positive - negative)
```


##feature extraction

```{r}
feature<-data.frame(punc=Q2.num$punc-Q1.num$punc)
feature$modal<-(Q2.num$modal-Q1.num$modal)/(Q1.num$word+Q2.num$word)
feature$length<-(Q2.num$word-Q1.num$word)/(Q1.num$word+Q2.num$word)
feature$neg<-(Q2.num$neg-Q1.num$neg)/(Q1.num$word+Q2.num$word)
feature$sentiment<-Q2.sentiment$net-Q1.sentiment$net
feature$positive<-Q2.sentiment$positive-Q1.sentiment$positive
feature$negative<-Q2.sentiment$negative-Q1.sentiment$negative

write.csv(feature,file = "../output/feature_lyq.csv")
```


##combine features

```{r}
tfeature<-read.csv("../output/feature.csv",header = TRUE)[,-1]
tfeature<-cbind(tfeature,read.csv("../output/feature_yiwenci.csv",header = TRUE)[,-1])
tfeature<-cbind(tfeature,read.csv("../output/feature_lyq.csv",header = TRUE)[,-1])
tfeature<-cbind(tfeature,(read.csv("../output/parsing.Q1.csv",header = TRUE)-read.csv("../output/parsing.Q2.csv",header = TRUE))[,-1])
write.csv(tfeature,file="../output/allfeatures.csv")
```

##train model (gbm)

```{r}
if(!require("gbm")){
  install.packages("gbm")
}

library(gbm)

train <- function(dat_train, label_train, par=NULL){
  if(is.null(par)){
    depth <- 3
  } else {
    depth <- par$depth
  }
  
  fit_gbm <- gbm.fit(x=dat_train, y=label_train,
                     n.trees=1000,
                     distribution="bernoulli",
                     interaction.depth=depth, 
                     bag.fraction = 0.5,
                     verbose=FALSE)
  best_iter <- gbm.perf(fit_gbm, method="OOB", plot.it = FALSE)
  
  return(list(fit=fit_gbm, iter=best_iter))
}



test <- function(fit_train, dat_test){
  
  pred <- predict(fit_train$fit, newdata=dat_test, 
                  n.trees=fit_train$iter, type="response")
  
  return(as.numeric(pred> 0.5))
}

cv.function <- function(X.train, y.train, d, K){
  n <- length(y.train)
  n.fold <- floor(n/K)
  s <- sample(rep(1:K, c(rep(n.fold, K-1), n-(K-1)*n.fold)))  
  cv.error <- rep(NA, K)
  
  for (i in 1:K){
    train.data <- X.train[s != i,]
    train.label <- y.train[s != i]
    test.data <- X.train[s == i,]
    test.label <- y.train[s == i]
    
    par <- list(depth=d)
    fit <- train(train.data, train.label, par)
    
    pred <- test(fit, test.data)  
    cv.error[i] <- mean(pred != test.label)  
    
  }			
  return(mean(cv.error))
  
}



k<-3
cl <- makeCluster(getOption("cl.cores", 8))
err<-parLapply(cl, 1:16, cv.function,X.train=tfeature, y.train=label, K=k)
plot(err)
d<-which.min(err)
gbmbase <- gbm.fit(x=tfeature, y=label,
                     n.trees=1000,
                     distribution="bernoulli",
                     interaction.depth=d, 
                     bag.fraction = 0.5,
                     verbose=FALSE)
save(gbmbase,file="../output/gbmmodel.RData")
```


