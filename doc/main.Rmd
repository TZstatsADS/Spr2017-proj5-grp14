---
title: "Main Script - Project 5"
author: "Group 14: Boya Zhao, Liangbin Chen, Yaqin Li, Yi Jiang"
date: "4/27/2017"
output: pdf_document
---

# Part 1: Problem discription

Where else but Quora can a physicist help a chef with a math problem and get cooking tips in return? Quora is a place to gain and share knowledge—about anything. It’s a platform to ask questions and connect with people who contribute unique insights and quality answers. This empowers people to learn from each other and to better understand the world.

Over 100 million people visit Quora every month, so it's no surprise that many people ask similarly worded questions. Multiple questions with the same intent can cause seekers to spend more time finding the best answer to their question, and make writers feel they need to answer multiple versions of the same question. Quora values canonical questions because they provide a better experience to active seekers and writers, and offer more value to both of these groups in the long term.

So we tackled this natural language processing problem by applying advanced techniques to classify whether question pairs are duplicated or not. Doing so will make it easier to find high quality answers to questions resulting in an improved experience for Quora writers, seekers, and readers.

# Part 2: Data exploration

```{r}
setwd("~/Desktop/sem 2/Applied data science/Spr2017-proj5-grp14")
Data <- read.csv("../data/train.csv", header = TRUE)
head(Data)
```

# Part 3: How we did it

## Constructed feature

We constructed four features: 

Similarity: counted the number of words in each question, measured the similarity between each pair of questions, including the number of same verbs and nouns.

Interrogative word: which pair of interrogative words showed up in each pair of questions.

Parsing: measured the difference of parsing result in each pair of questions.

Basic properties: determine the amount of punctuations, modals and negative words, such as "non" and "not".

Sentiment analysis: analysis the sentiment components in the sentences (based on the "bing" vocabulary).

```{r}
feature <- read.csv("../output/allfeatures.csv", header = TRUE)
head(feature)
label <- Data$is_duplicate
```

## Model training and results

### Random forest

![Figure 1: GBM Cross Validation Results](~/Desktop/sem 2/Applied data science/Spr2017-proj5-grp14/output/cv_rf.png)

```{r}
load("../output/err_rf_test.RData")
load("../output/rf_pred.RData")
```


### adaboost

```{r}

```

### GBM

```{r}
load("../output/gbmresults.RData")
err_gbm_test <- results[[1]]
gbm_pred <- results[[2]]
```

# Part 4: Comparison


```{r}
source('../lib/evaluation_measures.R', local = T)
result.class.c <- prediction
matching_matrix_rf <- matching_matrix(as.numeric(label[-train.index]), rf_predict)
performance_rf <- performance_statistics(matching_matrix_rf)
matching_matrix_ada <- matching_matrix(as.numeric(label[-train.index]),result.class.j)
performance_ada <- performance_statistics(matching_matrix_ada)
matching_matrix_gbm <- matching_matrix(as.numeric(label[-train.index]), gbm_pred)
performance_gbm <- performance_statistics(matching_matrix_gbm)

compare_df <- data.frame(method = c("Random Forest", "Adaboost", "GBM"),
                         test.error = c(err_rf_test, err_gbm_test),
                         precision = c(performance_rf$precision, performance_ada$precision, performance_gbm$precision),
                         recall = c(performance_rf$recall, performance_ada$recall, performance_gbm$recall),
                         f1 = c(performance_rf$f1, performance_ada$f1, performance_gbm$f1),
                         accuracy = c(performance_rf$accuracy, performance_ada$accuracy, performance_gbm$accuracy))
kable(compare_df,caption = "Comparision of performance for two clustering methods", digits = 2)


```






